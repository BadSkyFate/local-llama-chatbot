# local-llama-chatbot
Offline LLaMA 2 chatbot running locally with llama.cpp and a quantized GGUF model.

### ðŸ§  Credits
This project is built on llama.cpp by @ggerganov, an open-source C/C++ implementation of LLaMA inference that makes running large language models locally both possible and efficient.


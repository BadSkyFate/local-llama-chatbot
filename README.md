# local-llama-chatbot
Offline LLaMA 2 chatbot running locally with llama.cpp and a quantized GGUF model.

### üß† Credits
This project is built on llama.cpp by @ggerganov, an open-source C/C++ implementation of LLaMA inference that makes running large language models locally both possible and efficient.

üíª Tested System Specs
This project was successfully built and tested on the following local machine configuration:

OS: Windows 10

CPU: Intel CPU with AVX2 and FMA support

GPU: NVIDIA GeForce RTX 2060

RAM: 16GB

Python: 3.12.9 (via virtual environment)

Terminal: Git Bash

Model Format: GGUF

Model Tested: llama-2-7b-chat.Q4_K_M.gguf

‚ö†Ô∏è Performance may vary depending on your hardware and setup.


ü§î # Personal Reception:
Used chatgpt to set up most of this.
Chat-GPT experienced major convolusions and confusion during the entire process.
Very frustrating to use chatgpt for this.
Setting this up took me 2 weeks.

However, after it started to work, I was blown away.
ü§© Highly recommend.

# üôè I cannot thank the original devs enough! Thank you!
